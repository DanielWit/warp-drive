{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f77e5b",
   "metadata": {},
   "source": [
    "# Fast Multi-agent Reinforcement Learning on a GPU using WarpDrive and Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade13c1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0c7e8",
   "metadata": {},
   "source": [
    "This tutorial provides a demonstration of a multi-agent Reinforcement Learning (RL) training loop with [WarpDrive](https://github.com/salesforce/warp-drive). WarpDrive is a flexible, lightweight, and easy-to-use RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit). Using the extreme parallelization capability of GPUs, it enables [orders-of-magnitude faster RL](https://arxiv.org/abs/2108.13976) compared to common implementations that blend CPU simulations and GPU models. WarpDrive is extremely efficient as it runs simulations across multiple agents and multiple environment replicas in parallel and completely eliminates the back-and-forth data copying between the CPU and the GPU.\n",
    "\n",
    "We have integrated WarpDrive with the [Pytorch Lightning](https://www.pytorchlightning.ai/) framework, which greatly reduces the trainer boilerplate code, and improves training flexibility.\n",
    "\n",
    "Below, we demonstrate how to use WarpDrive and PytorchLightning together to train a game of [Tag](https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py) where multiple *tagger* agents are trying to run after and tag multiple other *runner* agents. As such, the Warpdrive framework comprises several utility functions that help easily implement any (OpenAI-)*gym-style* RL environment, and furthermore, provides quality-of-life tools to train it end-to-end using just a few lines of code. You may familiarize yourself with WarpDrive with the help of these [tutorials](https://github.com/salesforce/warp-drive/tree/master/tutorials).\n",
    "\n",
    "We invite everyone to **contribute to WarpDrive**, including adding new multi-agent environments, proposing new features and reporting issues on our open source [repository](https://github.com/salesforce/warp-drive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37d334",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c733ee",
   "metadata": {},
   "source": [
    "This notebook requires the `rl-warp-drive` as well as the `pytorch-lightning` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77cc5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement 1.4.5 (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for 1.4.5\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet rl_warp_drive >= 1.4.5 pytorch_lightning >= 1.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50124771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'warp_drive.training.lightning_trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1f2ea26809f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexample_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_continuous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_continuous\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTagContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarp_drive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwarp_drive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWarpDriveModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPerfStatsCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarp_drive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_and_push_data_placeholders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warp_drive.training.lightning_trainer'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.lightning_trainer import WarpDriveModel, PerfStatsCallback\n",
    "from warp_drive.training.utils.data_loader import create_and_push_data_placeholders\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR.\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ad0a4",
   "metadata": {},
   "source": [
    "# Specify a set of run configurations for your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d97e74",
   "metadata": {},
   "source": [
    "The run configuration is a dictionary comprising the environment parameters, the trainer and the policy network settings, as well as configurations for saving.\n",
    "\n",
    "For our experiment, we consider an environment wherein 5 taggers and 100 runners play the game of [Tag](https://github.com/salesforce/warp-drive/blob/master/example_envs/tag_continuous/tag_continuous.py) on a 20x20 plane. The game lasts 500 timesteps. Each agent chooses it's acceleration and turn actions at every timestep, and we use mechanics to determine how the agents move over the grid. When a tagger gets close to a runner, the runner is tagged, and is eliminated from the game. For the configuration below, The taggers and runners have the same skill level, i.e., the runners can move just as fast as the taggers.\n",
    "\n",
    "The sequence of snapshots below shows a sample realization of the game with randomly chosen agent actions. The 5 taggers are marked in pink, while the blue agents are the runners. The snapshots are taken at 1) the beginning of the episode, 2) step 250, and 3) end of the episode. Only 45% of the runners remain at the end of the episode.\n",
    "<img src=\"assets/tag_continuous_training/t=1.png\" width=\"250\" height=\"250\"/> <img src=\"assets/tag_continuous_training/t=250.png\" width=\"250\" height=\"250\"/> <img src=\"assets/tag_continuous_training/t=499.png\" width=\"250\" height=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5818e4a",
   "metadata": {},
   "source": [
    "We train the agents using 100 environments or simulations running in parallel. With WarpDrive, each simulation runs on sepate GPU blocks.\n",
    "\n",
    "There are two separate policy networks used for the tagger and runner agents. Each network is a fully-connected model with two layers each of 256 dimensions. We use the Advantage Actor Critic (A2C) algorithm for training. WarpDrive also currently provides the option to use the Proximal Policy Optimization (PPO) algorithm instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    name=\"tag_continuous\",\n",
    "    # Environment settings.\n",
    "    env=dict(\n",
    "        num_taggers = 5,  # number of taggers in the environment\n",
    "        num_runners = 100,  # number of runners in the environment\n",
    "        grid_length = 20.0,  # length of the (square) grid on which the game is played\n",
    "        num_acceleration_levels = 20,  # number of accelerate actions\n",
    "        num_turn_levels = 20,  # number of turn actions\n",
    "        episode_length = 500,  # episode length in timesteps\n",
    "        skill_level_tagger = 1.0,  # skill level for the tagger\n",
    "        skill_level_runner = 1.0,  # skill level for the runner\n",
    "    ),\n",
    "    # Trainer settings.\n",
    "    trainer=dict(\n",
    "        num_envs=100,  # number of environment replicas (number of GPU blocks used)\n",
    "        train_batch_size=25000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes=5000,  # total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    # Policy network settings.\n",
    "    policy=dict(\n",
    "        runner=dict(\n",
    "            to_train=True,  # flag indicating whether the model needs to be trained\n",
    "            algorithm=\"A2C\",  # algorithm used to train the policy\n",
    "            gamma=0.98,  # discount rate\n",
    "            lr=0.005,  # learning rate\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),  # policy model settings\n",
    "        ),\n",
    "        tagger=dict(\n",
    "            to_train=True,\n",
    "            algorithm=\"A2C\",\n",
    "            gamma=0.98,\n",
    "            lr=0.001,\n",
    "            model=dict(\n",
    "                type=\"fully_connected\", fc_dims=[256, 256], model_ckpt_filepath=\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    # Checkpoint saving setting.\n",
    "    saving=dict(\n",
    "        metrics_log_freq=100,  # how often (in iterations) to print the metrics\n",
    "        model_params_save_freq=5000,  # how often (in iterations) to save the model parameters\n",
    "        basedir=\"/tmp\",  # base folder used for saving\n",
    "        name=\"continuous_tag\",  # experiment name\n",
    "        tag=\"example\",  # experiment tag\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b1cdb",
   "metadata": {},
   "source": [
    "# Instantiate the WarpDrive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667474e",
   "metadata": {},
   "source": [
    "In order to instantiate the WarpDrive model, we first use an environment wrapper to specify that the environment needs to be run on the GPU (via the `use_cuda` flag). Also, agents in the environment can share policy models; so we specify a dictionary to map each policy network model to the list of agent ids using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapped environment object via the EnvWrapper.\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU).\n",
    "env_wrapper = EnvWrapper(\n",
    "    TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}\n",
    "\n",
    "wd_model = WarpDriveModel(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9567c",
   "metadata": {},
   "source": [
    "# Create the Lightning Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ab7e1",
   "metadata": {},
   "source": [
    "Next, we create the trainer for training the WarpDrive model. We add the `checkpoint` and the `performance stats` callbacks to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks.\n",
    "perf_stats_callback = PerfStatsCallback(\n",
    "    batch_size=wd_model.training_batch_size,\n",
    "    num_iters=wd_model.num_iters,\n",
    "    log_freq=run_config[\"saving\"][\"metrics_log_freq\"]\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(every_n_val_epochs=100, save_top_k=-1, save_last=False)\n",
    "\n",
    "# Instantiate the PytorchLightning trainer with the callbacks and the number of gpus.\n",
    "num_gpus = 1\n",
    "assert num_gpus <= torch.cuda.device_count(), f\"Only {torch.cuda.device_count()} GPU(s) are available!\"\n",
    "trainer = Trainer(\n",
    "    gpus=num_gpus,\n",
    "    callbacks=[checkpoint_callback, perf_stats_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d09e27",
   "metadata": {},
   "source": [
    "# Train the WarpDrive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077dcba2",
   "metadata": {},
   "source": [
    "Finally, we invoke training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba66c41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(wd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc20e5",
   "metadata": {},
   "source": [
    "# TODO: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ab6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
