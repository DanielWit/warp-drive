{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baff84d",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba754b9",
   "metadata": {},
   "source": [
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda63add",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fe5fd",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebd3ce",
   "metadata": {},
   "source": [
    "In this tutorial, we describe how to\n",
    "- Use the WarpDrive framework to perform end-to-end training of multi-agent reinforcement learning (RL) agents.\n",
    "- Visualize the behavior using the trained policies.\n",
    "\n",
    "In case you haven't familiarized yourself with WarpDrive, please see the other tutorials we have prepared for you\n",
    "- [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "- [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "- [WarpDrive reset and log controller](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "\n",
    "Please also see our [tutorial](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md) on creating your own RL environment in CUDA C. Once you have your own environment in CUDA C, this tutorial explains how to integrate it with the WarpDrive framework to perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b31c7",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccca4cb",
   "metadata": {},
   "source": [
    "You can install the warpdrive package using\n",
    "\n",
    "- the pip package manager OR\n",
    "- by cloning the warp_drive package and installing the requirements (we shall use this when running on Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8608de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! git clone https://github.com/salesforce/warp-drive.git\n",
    "    % cd warp-drive\n",
    "    ! pip install -e .\n",
    "    % cd tutorials\n",
    "else:\n",
    "    ! pip install rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dca5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.training.models.fully_connected import FullyConnected\n",
    "\n",
    "from example_envs.tag_continuous.tag_continuous import TagContinuous\n",
    "from utils.generate_rollout_animation import generate_tag_env_rollout_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from IPython.display import HTML\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64587609",
   "metadata": {},
   "source": [
    "# Training the continuous version of Tag with WarpDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b97fca",
   "metadata": {},
   "source": [
    "We will now explain how to train your environments using WarpDrive in just a few steps. For the sake of exposition, we consider the continuous version of Tag.\n",
    "\n",
    "For your reference, there is also an example end-to-end RL training script [here](https://github.com/salesforce/warp-drive/blob/master/warp_drive/training/example_training_script.py) that contains all the steps below. It can use to set up your own custom training pipeline. Invoke training by using\n",
    "```shell\n",
    "python warp_drive/training/example_training_script.py --env <ENV-NAME>\n",
    "```\n",
    "where `<ENV-NAME>` can be `tag_gridworld` or `tag_continuous` (or any new env that you build)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e9fbf",
   "metadata": {},
   "source": [
    "## Step 1: Specify a set of run configurations for your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993c67f",
   "metadata": {},
   "source": [
    "In order to run the training for these environments, we first need to specify a *run config*, which comprises the set of environment, training, and model parameters.\n",
    "\n",
    "Note: there are also some default configurations in 'warp_drive/training/run_configs/default_configs.yaml', and the run configurations you provide will override them.\n",
    "\n",
    "For this tutorial, we will use the configuration [here](assets/tag_continuous_training/run_config.yaml). Specifically, we'll use $5$ taggers and $100$ runners in a $20 \\times 20$ square grid. The taggers and runners have the same skill level, i.e., the runners can move just as fast as the taggers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the run config.\n",
    "with open(\"assets/tag_continuous_training/run_config.yaml\", encoding=\"utf8\") as fp:\n",
    "    run_config = yaml.safe_load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f275123",
   "metadata": {},
   "source": [
    "## Step 2: Create the environment object using WarpDrive's envWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d039bd6",
   "metadata": {},
   "source": [
    "### Important! Ensure that 'use_cuda' is set to True below (in order to run the simulation on the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe91c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_wrapper = EnvWrapper(\n",
    "    env_obj=TagContinuous(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    use_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149af29",
   "metadata": {},
   "source": [
    "Creating the env wrapper initializes the CUDA data manager and pushes some reserved data arrays to the GPU. It also initializes the CUDA function manager, and loads some WarpDrive library CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229844c6",
   "metadata": {},
   "source": [
    "## Step 3: Specify a mapping from the policy to agent indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2d5c6",
   "metadata": {},
   "source": [
    "Next, we will need to map each trainable policy to the agent indices that are using it. As such, we have the tagger and runner policies, and we will map those to the corresponding agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ade2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tag_to_agent_id_map = {\n",
    "    \"tagger\": list(env_wrapper.env.taggers),\n",
    "    \"runner\": list(env_wrapper.env.runners),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c129e4f",
   "metadata": {},
   "source": [
    "Note that if you wish to use just a single policy across all the agents (or if you wish to use many other policies), you will need to update the run configuration as well as the policy_to_agent_id_mappping.\n",
    "\n",
    "For example, for using a shared policy across all agents (say `shared_policy`), for example, you can just use the run configuration as\n",
    "```python\n",
    "    \"policy\": {\n",
    "        \"shared_policy\": {\n",
    "            \"to_train\": True,\n",
    "            ...\n",
    "        },\n",
    "    },\n",
    "```\n",
    "and also set all the agent ids to use this shared policy\n",
    "```python\n",
    "    policy_tag_to_agent_id_map = {\n",
    "        \"shared_policy\": np.arange(envObj.env.num_agents),\n",
    "    }\n",
    "```\n",
    "\n",
    "**Importantly, make sure the `policy` keys and the `policy_tag_to_agent_id_map` keys are identical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfadf07",
   "metadata": {},
   "source": [
    "## Step 4: Create the Trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3ac50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    env_wrapper,\n",
    "    run_config,\n",
    "    policy_tag_to_agent_id_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557d496",
   "metadata": {},
   "source": [
    "When the trainer object is created, all the relevant data arrays (e.g., \"loc_x\", \"loc_y, \"speed\") are pushed from the CPU to the GPU. Additionally, the observation, reward, action and done flag data arrays are also pushed. As training happens, all these arrays are update in-place, and there's no data transferred back to the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc60512",
   "metadata": {},
   "source": [
    "# Visualizing the trainer policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200446d1",
   "metadata": {},
   "source": [
    "## Visualizing an episode roll-out before training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28415eb8",
   "metadata": {},
   "source": [
    "Let us visualize an episode rollout before training begins. Note that at any time during training, we can fetch the data arrays on the GPU using the trainer API `fetch_episode_states`.\n",
    "\n",
    "Below, we fetch the state arrays pertaining to agent locations and indicators on which agents are still active in the game, and will use these to visualize an episode roll-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states = trainer.fetch_episode_states(\n",
    "    [\n",
    "        \"loc_x\",\n",
    "        \"loc_y\",\n",
    "        \"still_in_the_game\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(env_wrapper.env, episode_states)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af2085",
   "metadata": {},
   "source": [
    "In the visualization above, the large purple dots represent the taggers, while the smaller blue dots represent the runners. Before training, the runners and taggers move around randomly, and that only results in about half the runners getting tagged, just by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35241f",
   "metadata": {},
   "source": [
    "## Step 5: Perform training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84747a45",
   "metadata": {},
   "source": [
    "Training is performed by calling trainer.train(). We run training for just a few episodes, as specified in the run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b968e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c82f50",
   "metadata": {},
   "source": [
    "As training happens, we log the speed performance numbers and the metrics for all the trained policies every `metrics_log_freq` iterations. The training results and the model checkpoints are also saved on a timely (as specified in the run configuration parameters `model_params_save_freq`) basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d8e58",
   "metadata": {},
   "source": [
    "## Visualize an episode-rollout after training (for about 20M steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51991d1",
   "metadata": {},
   "source": [
    "We can also initialize the trainer model parameters using saved model checkpoints via the `load_model_checkpoint` API. With this, we will be able to fetch the episode states for a trained model, for example. We will now visualize an episode roll-out using trained tagger and runner policy model weights (trained for 20M steps), that are located in [this](assets/tag_continuous_training/) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546838d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model_checkpoint(\n",
    "    {\n",
    "        \"tagger\": \"assets/tag_continuous_training/tagger_20000000.state_dict\",\n",
    "        \"runner\": \"assets/tag_continuous_training/runner_20000000.state_dict\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states = trainer.fetch_episode_states(\n",
    "    [\n",
    "        \"loc_x\",\n",
    "        \"loc_y\",\n",
    "        \"still_in_the_game\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47045f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(env_wrapper.env, episode_states)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5999b8",
   "metadata": {},
   "source": [
    "After training, the runners learn to run away from the taggers, and the taggers learn to chase them; there are some instances where we see that taggers also team up to chase and tag the runners. Eventually, (almost) all the runners are caught now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the trainer to clear up the CUDA memory heap\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208fa8b",
   "metadata": {},
   "source": [
    "You've now seen the entire end-to-end multi-agent RL pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e75f6f",
   "metadata": {},
   "source": [
    "# Learn More and Explore our Tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1a7b8",
   "metadata": {},
   "source": [
    "For your reference, all our tutorials are here:\n",
    "- [A simple end-to-end RL training example](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)\n",
    "- [WarpDrive basics](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1-warp_drive_basics.ipynb)\n",
    "- [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb)\n",
    "- [WarpDrive reset and log](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "- [Creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md)\n",
    "- [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
